<!doctype html>
<html>
<head>
<meta charset="UTF-8">
<title>AWRL 2019</title>
<style>
body {
	font-family: "Helvetica Neue", Helvetica, Roboto, Arial, sans-serif;
	font-size: 14px;
	text-align: left;
}
h3 {
	font-size: 18px;
	color: #5555bb;
	margin-top:25px;
}
a {
    color: #1779ba;
    text-decoration: none;
    cursor: pointer;
}
.small{
	font-size: 11px;
	color: #444;
	margin-left: 10px;
	margin-bottom: 5px;
}
</style>
</head>

<body>
<table cellpadding="0" cellspacing="0" border="0" width="800" align="center">
  <tr>
    <td align="left" width="700"> <span style="font-size:24px; color:#5555bb">The 4th Asian Workshop on Reinforcement Learning (AWRL'19)</span><br/>
      Oct. 13, Beijing International Convention Center (BICC), Beijing, China <br/> </td>
    <td align= "right"><a href="http://www.adai.ai"><img src="http://www.adai.ai/img/logo.png" width="150px"></a></td>
  </tr>
  <tr>
    <td colspan="2" align="center"><br/><hr size="1">
      <a href="#invited" target="_self">Invited Speakers</a> | 
      <a href="#program" target="_self">Program</a> |
      <a href="#organize" target="_self">Organization</a> |
      <a href="#sponsor" target="_self">Sponsors</a> |
      <a href="http://awrl.cc">About AWRL</a> <br/><hr size="1">
      <br/></td>
  </tr>
  <tr>
    <td colspan="2" width="800">
<p>(<font color="blue">Registration: Attendees should <a href="http://www.adai.ai/dai/registration.html" target="_blank">register to DAI</a>. 参会注册：参会代表请在<a href="http://www.adai.ai/dai/registration.html" target="_blank">DAI注册</a></font>)</p>
<p>The Asian Workshop on Reinforcement Learning (AWRL) focuses on both theoretical foundations, models, algorithms, and practical applications. We intend to make this an exciting event for researchers and practitioners in RL worldwide as a forum for the discussion of open problems, future research directions and application domains of RL. </p>
      <p>AWRL 2019 (in conjunction with <a href="http://www.adai.ai" target="_blank">DAI 2019</a>) will consist of academic and industrial keynote talks, and invited paper presentations, over a one-day period.</p>
      <h3><a name="invited"></a>Invited Speakers (sorted in alphabetical order)</h3>
      <strong>Junqi Jin</strong>, Alibaba Group<br/>
      <div class="small">Dr. Junqi Jin is from Precision Orientation Technology Department at Alibaba Group, where his main research interests lie in machine learning, mechanism design applied to online advertising and recommendation. Junqi holds a PhD (2016), a Bachelor of Engineering (2011) and a Bachelor of Economics (2011) from Tsinghua University. Junqi has published research papers in IEEE TPAMI, TITS, TNNLS, KDD and CIKM.
      </div>
      <strong>Paul Weng</strong>, UM-SJTU Joint Institute<br>
      <div class="small">Paul Weng is currently a tenure-track assistant professor at UM-SJTU Joint Institute. Previously, he was a faculty at SYSU-CMU Joint Institute of Engineering from 2015 to 2017. During 2015, he was a visiting faculty at Carnegie Mellon University (CMU). Before that, he was an associate professor in computer science at Sorbonne University (Pierre and Marie Curie University, UPMC), Paris. He received his Master in 2003 and his Ph.D. in 2006, both at UPMC. Before joining academia, he graduated from ENSAI (French National School in Statistics and Information Analysis) and worked as a financial quantitative analyst in London. His main research work lies in artificial intelligence and machine learning. Notably, it focuses on adaptive control (reinforcement learning, Markov decision process) and multiobjective optimization (compromise programming, fair optimization).
      </div>
      <strong>Chongjie Zhang </strong>,  Institute for Interdisciplinary Information Sciences at Tsinghua University<br>
      <div class="small">Chongjie Zhang is an Assistant Professor in the Institute for Interdisciplinary Information Sciences at Tsinghua University. Before joining the faculty, he was a postdoctoral associate in the Computer Science and Artificial Intelligence Lab (CSAIL) at MIT. He received his Ph.D. in Computer Science from University of Massachusetts at Amherst in 2011. His research interests span reinforcement learning, multi-agent systems, and robotics.
      </div>
      <strong>Weinan Zhang</strong>, Shanghai Jiao Tong University<br>
      <div class="small">Weinan Zhang is now a tenure-track assistant professor in Department of Computer Science and John Hopcroft Center for Computer Science, Shanghai Jiao Tong University. His research interests include deep reinforcement learning, unsupervised learning and the applications on various big data mining scenarios. Weinan earned his Ph.D. from University College London in 2016 and B.Eng. from ACM Class of Shanghai Jiao Tong University in 2011. He was selected as one of the 20 rising stars of KDD research community in 2016 by Microsoft Research and won the ACM rising star award (Shanghai chapter) and DAMO young scholar award in 2018. His papers won the best paper honorable mention award in SIGIR 2017 and the best paper award in DLP-KDD 2019. 
      </div>
      <strong>Li Zhao</strong>, Microsoft Research Asia (MSRA)<br>
      <div class="small">Li Zhao is currently a Senior Researcher in Machine Learning Group, Microsoft Research Asia (MSRA). She obtained her PhD degree majoring in Computer Science in July, 2016, from Tsinghua University, supervised by Professor Xiaoyan Zhu. Her research interests mainly lie in deep learning and reinforcement learning, and their applications for text mining, finance, game and operations research. 
      </div>
      
      <h3><a name="program"></a>Program      </h3>
      <!--------
      <table width="100%" border="0" cellspacing="10px" cellpadding="0">
        <tr>
          <td width="120px" valign="top" bgcolor="#CCCCCC">8:50-9:00</td>
          <td bgcolor="#CCCCCC">Opening</td>
        </tr>
        <tr>
          <td valign="top">9:00-9:40</td>
          <td>Invited talk 1 by <strong>Lucian Busoniu</strong><br>
          <a href="2018/awrl18_handout.pdf" target="_blank">Forward-search optimistic planning with convergence guarantees in continuous-action MDPs</a><br/>
          <div class="small">We discuss an optimistic planning method to run a forward search for continuous-action sequences in deterministic MDPs. The method iteratively refines the most promising adaptive-horizon hyperboxes in the space of infinite-horizon action sequences. Under Lipschitz conditions on the dynamics and rewards, we obtain a convergence rate to the optimal solution as the simulation budget increases. Real-time, receding-horizon control results illustrate the method in practice.</div>
          </td>
        </tr>
        <tr>
          <td valign="top">9:40-10:00</td>
		<td>Paper talk 1 by <strong>Guangxiang Zhu</strong><br/>
			Object-Oriented Dynamics Predictor
		</td>
        </tr>
        <tr>
          <td valign="top" bgcolor="#CCCCCC">10:00-10:20</td>
          <td bgcolor="#CCCCCC">Coffee break</td>
        </tr>
        <tr>
          <td valign="top">10:20-11:00</td>
          <td>Invited talk 2 by <strong>Shane Shixiang  Gu</strong><br/>
          <a href="2018/ShixiangGu.pdf" target="_blank">Deep Reinforcement Learning Toward Robotics</a><br/>
          <div class="small">Deep reinforcement learning (RL) has shown promising results for learning complex sequential decision-making behaviors in various environments from computer games, the game of Go, to simulated humanoids. However, most successes have been exclusively in simulation, and results in real-world applications such as robotics are limited, largely due to poor sample efficiency of typical deep RL algorithm and other challenges. In this talk, I present essential components for deep reinforcement learning in the wild. First, I will discuss methods improve performance and sample efficiency of the core RL algorithms, blurring the boundaries among classic model-based RL, off-policy and on-policy model-free RL. In the latter part, I illustrate other practical challenges for enabling autonomous learning agents in the real world, particularly that current RL formulations require constant human interventions for safety, resets, and reward engineering, and do not scale to learn diverse skills. I present our recent work to address those challenges and show pathways to achieve continually learning robots in the real world.</div></td>
        </tr>
        <tr>
          <td valign="top">11:00-11:20</td>
		<td>Paper talk 2 by <strong>Yue Wang</strong><br/>
		Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov Setting</td>
        </tr>
        <tr>
          <td valign="top">11:20-12:00</td>
          <td>Invited talk 3 by <strong>Minlie Huang</strong><br/>
          <a href="2018/reinforcement%20learning-in-NLP-Huangminlie-%40AML-RLworkshop-long.pdf" target="_blank">Reinforcement Learning in Natural Language Processing and Search</a><br/>
          <div class="small">Deep reinforcement learning has received much attention since the success of Alpha GO/Zero. In this talk, the speaker will present his research attempts on how deep reinforcement learning can be applied in solving natural language processing or search problems including discovering text structures, removing noise instances, correcting noisy data labels, and optimizing online, complex, and dynamic search systems. In these works, the speaker will demonstrate how typical NLP/Search problems can be formulated as sequential decision problems. These works share in common that under the setting of weak or indirect supervision, reinforcement learning performs well by leveraging two properties: the nature of trial-and-error with probabilistic exploration, and reward design that captures prior knowledge or domain expertise knowledge. The speaker will also share his experiences on how to make RL succeed in solving NLP/Search problems.</div></td>
        </tr>
        <tr>
          <td bgcolor="#CCCCCC"><p>Lunch</p></td>
          <td bgcolor="#CCCCCC">&nbsp;</td>
        </tr>
        <tr>
          <td valign="top">13:00-13:40</td>
          <td>Invited talk 4 by <strong>Gergely Neu</strong><br/>
          <a href="2018/GergelyNeu.pdf" target="_blank">A unified view of entropy-regularized Markov decision processes</a><br/>
          <div class="small">Entropy regularization, while a standard technique in the online
learning toolbox, has only been recently discovered by the
reinforcement learning community: In recent years, numerous new
reinforcement learning algorithms have been derived using this
principle, largely independently of each other. So far, a general
framework for these algorithms has remained elusive. In this work, we
propose such a general framework for entropy-regularized
average-reward reinforcement learning in Markov decision processes
(MDPs). Our approach is based on extending the linear-programming
formulation of policy optimization in MDPs to accommodate convex
regularization functions. Our key result is showing that using the
conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling
the Bellman optimality equations. This result enables us to formalize
a number of state-of-the-art entropy-regularized reinforcement
learning algorithms as approximate variants of Mirror Descent or Dual
Averaging, and thus to argue about the convergence properties of these
methods. In particular, we show that the exact version of the TRPO
algorithm of Schulman et al. (2015) actually converges to the optimal
policy, while the entropy-regularized policy gradient methods of Mnih
et al. (2016) may fail to converge to a fixed point.</div></td>
        </tr>
        <tr>
          <td valign="top">13:40-14:00</td>
		<td>Paper talk 3 by <strong>Yan Zheng</strong><br/>
		A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents</td>
        </tr>
        <tr>
          <td valign="top">14:00-14:40</td>
          <td>Invited talk 5 by <strong>Matteo Pirotta</strong><br/>
          <a href="2018/Matteo.pdf" target="_blank">Exploration Bonus for Regret Minimization in Reinforcement Learning</a><br/>
          <div class="small">A sample-efficient RL agent must trade off the exploration needed to collect information about the environment, and the exploitation of the experience gathered so far to gain as much reward as possible. A popular strategy to deal with the exploration-exploitation dilemma (i.e., minimize regret) is to follow the optimism in the face of uncertainty (OFU) principle.<br>
We present SCAL+, an optimistic algorithm that relies on an exploration bonus to efficiently balance exploration and exploitation in the infinite-horizon undiscounted setting. We show that all the exploration bonuses that were previously introduced in the RL literature explicitly exploit some form of prior knowledge associated to the specific setting (i.e., discounted of finite-horizon problems). In the infinite-horizon undiscounted case, there is no predefined parameter playing such a role. This makes the design of an exploration bonus very challenging. To overcome this limitation, we make the common assumption that the agent knows an upper-bound c on the span of the optimal bias.<br>
We discuss the connections between the different settings and we prove that SCAL+ achieves the same theoretical guarantees of standard approaches (e.g., UCRL), with a much smaller computational complexity.</div></td>
        </tr>
        <tr>
          <td valign="top">14:40-15:00</td>
		<td>Paper talk 4 by <strong>Hideaki Kano</strong><br/>
		Good Arm Identification via Bandit Feedback</td>
        </tr>
        <tr>
          <td valign="top">15:00-15:20</td>
		<td>Paper talk 5 by <strong>Siyuan Li</strong><br/>
		  An Optimal Online Method of Selecting Source Policies for Reinforcement Learning
	  </td>
        </tr>
        <tr>
          <td valign="top" bgcolor="#CCCCCC">15:20-15:30</td>
          <td bgcolor="#CCCCCC">Close</td>
        </tr>
      </table>
      --------!>
      
      <h3><a name="organize"></a>Organization Committee</h3>
      <p>Chao Yu,  Dalian University of Technology, China<br>
        Jianye Hao, Tianjing University, China</p>
        Yang Yu, Nanjing University, China <br>
      <h3><a name="sponsor"></a>Sponsors</h3>
      <p ><img src="http://lamda.nju.edu.cn/logo/Normal%20(179x85)/normal.png" width="100"><br>
        LAMDA Group, National Key Laboratory for Novel Software Technology, Nanjing University </td>
  </tr>
</table>
</body>
</html>
</body>
</html>
